{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTL7sTCPd7sd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "from google.colab import drive\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCBPKw-N0TA4",
        "outputId": "39fbfb34-9c9e-4d2b-ca42-5112a44cd197"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount google drive to colab notebook\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Specify dataset location in mounted drive\n",
        "DATA = '/content/drive/MyDrive/ML_project_two/ML_patent_data_filtered/final_dataset/features/features.csv'\n",
        "\n",
        "# Specify word2vec datasets locations\n",
        "WORD2VEC_ROOT = '/content/drive/MyDrive/ML_project_two/word_vector/'\n",
        "INDEX = WORD2VEC_ROOT + 'index.txt'\n",
        "WORD2VEC_FLOAT = WORD2VEC_ROOT + 'float/word_vector_NUMBER_float.csv'\n",
        "\n",
        "# Use gpu if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxdEi8XwCZjf"
      },
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZtwv6FhCcU-"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(y_pred, y_test):\n",
        "    # Round float predictions to the respective classes\n",
        "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
        "  \n",
        "    # Compute true/false positives/negatives\n",
        "    tp = torch.logical_and(y_pred_tag == 1, y_test == 1).sum()\n",
        "    fp = torch.logical_and(y_pred_tag == 1, y_test == 0).sum()\n",
        "    tn = torch.logical_and(y_pred_tag == 0, y_test == 0).sum()\n",
        "    fn = torch.logical_and(y_pred_tag == 0, y_test == 1).sum()\n",
        "\n",
        "    # Compute accuracy, precision, recall and f1 score\n",
        "    acc = ((tp + tn) / (tp + tn + fp + fn))\n",
        "    prec = tp / (tp + fp)\n",
        "    rec = tp / (tp + fn)\n",
        "    f1 = 2 * (prec * rec) / (prec + rec)\n",
        "    return acc, prec, rec, f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRcQEDgHocPC"
      },
      "source": [
        "### Data Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10IIBHMEpnan"
      },
      "outputs": [],
      "source": [
        "class PatentDataset(Dataset):\n",
        "    def __init__(self):\n",
        "\n",
        "        print(\"Loading attorney and inventor data...\")\n",
        "        # Load csv file\n",
        "        df = pd.read_csv(DATA, delimiter=',', \n",
        "                              usecols=['application_number',\n",
        "                                       'approved',\n",
        "                                       'examiner_art_unit',\n",
        "                                       'inventors_number',\n",
        "                                       'inventors_avg_toal_patents',\n",
        "                                       'attorneys_number',\n",
        "                                       'attorneys_avg_total_patents'])\n",
        "\n",
        "        # Initialize a dataframe to contain word2vec information, which will later\n",
        "        # be merged with the attorney/inventor dataframe \n",
        "        w2v_df = pd.DataFrame()\n",
        "\n",
        "        # There are 43 files containing w2v data\n",
        "        # Only load some to save memory\n",
        "        w2v_idx_subset = np.random.default_rng(seed=1).choice(43, size=5, replace=False)\n",
        "        for i in w2v_idx_subset:\n",
        "            # Build file name and load csv\n",
        "            w2v_filename = WORD2VEC_FLOAT.replace('NUMBER', str(i))\n",
        "\n",
        "            # Read csv and append to general w2v dataframe\n",
        "            print(f\"Loading {w2v_filename}...\")\n",
        "            w2v_sub_df = pd.read_csv(w2v_filename)\n",
        "            w2v_df = w2v_df.append(w2v_sub_df)\n",
        "\n",
        "        # Perform an inner join between the word2vec dataset and the attorney/\n",
        "        # inventors features dataset.\n",
        "        df = df.merge(w2v_df, how='inner', on='application_number')\n",
        "        \n",
        "        # Drop application_number feature\n",
        "        df = df.drop(labels='application_number', axis=1)\n",
        "        \n",
        "        # One-hot encode the art unit feature\n",
        "        df = pd.get_dummies(df, columns=['examiner_art_unit'])\n",
        "\n",
        "        # Convert the pandas dataframe into a numpy 2d matrix for performance\n",
        "        dataset = df.to_numpy(dtype='float32')\n",
        "\n",
        "        # Set length\n",
        "        self.len = dataset.shape[0]\n",
        "\n",
        "        # Set features and labels\n",
        "        self.x = dataset[:, 1:]\n",
        "        self.y = dataset[:, 0][:, None]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.x[index], self.y[index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ngI6sqq0TxC"
      },
      "source": [
        "### Net Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrQP4Axh0X7j"
      },
      "outputs": [],
      "source": [
        "class BinaryClassifier(nn.Module):\n",
        "    def __init__(self, D_in, H, D_out=1):\n",
        "        super(BinaryClassifier, self).__init__()\n",
        "        self.linear1 = nn.Linear(D_in, H)\n",
        "        self.linear2 = nn.Linear(H, D_out)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h_relu = self.linear1(x).clamp(min=0)\n",
        "        y_pred = self.linear2(h_relu)\n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nRFmtnCciA7"
      },
      "outputs": [],
      "source": [
        "class DeeperBinaryClassifier(nn.Module):\n",
        "    def __init__(self, D_in, H, D_out=1):\n",
        "        super(DeeperBinaryClassifier, self).__init__()\n",
        "        self.linear1 = nn.Linear(D_in, H)\n",
        "        self.linear2 = nn.Linear(H, H)\n",
        "        self.linear3 = nn.Linear(H, H)\n",
        "        self.linear4 = nn.Linear(H, D_out)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h_relu1 = self.linear1(x).clamp(min=0)\n",
        "        h_relu2 = self.linear2(h_relu1).clamp(min=0)\n",
        "        h_relu3 = self.linear3(h_relu2).clamp(min=0)\n",
        "        y_pred = self.linear4(h_relu3)\n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UW7XFJ352dG0"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data pre-process"
      ],
      "metadata": {
        "id": "Cea_neBvHzR1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqN6Cvlb6PsN"
      },
      "outputs": [],
      "source": [
        "TRAIN_SPLIT = 0.8\n",
        "LEARNING_RATE = 0.001\n",
        "TRAIN_BATCH_SIZE = 128\n",
        "TEST_BATCH_SIZE = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5Cb0GH-46Ft",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "217c5a20-cbfc-4bcd-d53d-36614218713c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading attorney and inventor data...\n"
          ]
        }
      ],
      "source": [
        "dataset = PatentDataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yl-b7OVz4_GF"
      },
      "outputs": [],
      "source": [
        "# Data pre-processing\n",
        "# Split data in training and test set\n",
        "train_size = int(len(dataset) * TRAIN_SPLIT)\n",
        "trainset, testset = random_split(dataset, [train_size, len(dataset)-train_size])\n",
        "\n",
        "# Initialize DataLoaders\n",
        "train_loader = DataLoader(trainset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(testset, batch_size=TEST_BATCH_SIZE, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Net creation and training"
      ],
      "metadata": {
        "id": "Subv5m2vH7tB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYJ4PhTy97Zt"
      },
      "outputs": [],
      "source": [
        "# Create the net\n",
        "net = BinaryClassifier(D_in=dataset.x.shape[1], H=1000)\n",
        "net.to(device)\n",
        "\n",
        "# Select loss function and optimizer\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Create lists to track performance metrics\n",
        "batch_loss = []\n",
        "batch_accuracy = []\n",
        "batch_precision = []\n",
        "batch_recall = []\n",
        "batch_f1 = []\n",
        "\n",
        "# Set number of epochs\n",
        "epochs = 1\n",
        "\n",
        "# Set net mode to training\n",
        "net.train()\n",
        "\n",
        "# Train\n",
        "for epoch in range(epochs):\n",
        "    for x_batch, y_batch in tqdm(train_loader):\n",
        "        # Move data to GPU if available\n",
        "        x_batch = x_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "        \n",
        "        # Reset tensor gradients from prev batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        y_pred = net(x_batch)\n",
        "\n",
        "        # Compute metrics\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "        acc, prec, rec, f1 = compute_metrics(y_pred, y_batch)\n",
        "        batch_loss.append(loss.item())\n",
        "        batch_accuracy.append(acc.item())\n",
        "        batch_precision.append(prec.item())\n",
        "        batch_recall.append(prec.item())\n",
        "        batch_f1.append(f1.item())\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set net mode to evaluation\n",
        "net.eval()\n",
        "\n",
        "# Create list to track accuracy\n",
        "test_acc = []\n",
        "test_prec = []\n",
        "test_rec = []\n",
        "test_f1 = []\n",
        "\n",
        "for x_batch, y_batch in tqdm(test_loader):\n",
        "    # Move data to GPU if available\n",
        "    x_batch = x_batch.to(device)\n",
        "    y_batch = y_batch.to(device)\n",
        "\n",
        "    # Calculate prediction\n",
        "    y_pred = net(x_batch)\n",
        "\n",
        "    # Compute metrics\n",
        "    acc, prec, rec, f1 = compute_metrics(y_pred, y_batch)\n",
        "    test_acc.append(acc.item())\n",
        "    test_prec.append(prec.item())\n",
        "    test_rec.append(rec.item())\n",
        "    test_f1.append(f1.item())"
      ],
      "metadata": {
        "id": "zRFWqrWYuRNF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "MLP2_NN_attempt2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}