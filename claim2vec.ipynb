{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1MZp7U2b2ro"
      },
      "source": [
        "# Generate Word Vectors by Doc2Vec\n",
        "We use gensim's open source library Doc2Vec to train our model.\n",
        "This note book takes the input generated from `feature-selection.ipynb` to load the patent claims as texts. And generate the corresponding word vectors to describe the claims. This notebook generates the following files.\n",
        "1. Training corpus and corresponding vocabulary set (both in `.txt`)\n",
        "2. gensim Doc2Vec model (stored as 4 files in `.model`, `.model.dv.vectors.npy`, `.model.dv.syn1neg.npy`, and `.model.wv.vectors.npy`.)\n",
        "3. word vector files (in `.csv`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiUfOrvgJVs5",
        "outputId": "154a46e4-188a-49db-b8f4-b94211df40d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (4.1.2)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# The default version of gensim is 3.6 and seems too old to support corpus_file document tagging\n",
        "# So we need to upgrade the version first\n",
        "!pip install gensim --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gs5Lu2xLJ1Vn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import ast\n",
        "\n",
        "from tqdm import tqdm\n",
        "from typing import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1pEQTRhjDIK"
      },
      "source": [
        "## Choosing the parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySpdjG0fD9lS"
      },
      "outputs": [],
      "source": [
        "# To select either to train a new Doc2Vec model\n",
        "# If True, train a new model\n",
        "# If False, directly use a old pre-trained model for vector inference\n",
        "train_new_model = False\n",
        "\n",
        "# Number of files with the same name format to load\n",
        "num_files = 43\n",
        "# Probability to sample applications when loading\n",
        "sample_prob = 0.1\n",
        "\n",
        "# Model training Specs\n",
        "vector_size = 450     # The vector dimension to represent any document. Only valid when training new model\n",
        "model_min_count = 5   # The minimum count of words when training. Any word appears less than this number of times is discarded during training.\n",
        "model_epochs = 10      # The number of epochs for training\n",
        "model_sample = 1e-4   # The ratio where high frequency words be downsampled during the training process\n",
        "\n",
        "# The path to the source data. Must contains (1) application number (2) claim number (3) claim texts\n",
        "source_path = '/content/drive/MyDrive/EPFL/ML/ML-Patent_data/ML_patent_data_filtered/final_dataset/claims/all_together_ordered_by_application_number'\n",
        "\n",
        "# The path where the model files are located. Same path for saving and loading the model.\n",
        "model_path = '/content/drive/MyDrive/EPFL/ML/ML-Patent_data/claim_with_vector/sampled/model_sampled.model'\n",
        "\n",
        "# The path to output the post-processed dataset, where each row contains\n",
        "# (1) application number (in integer)\n",
        "# (2) word count (in integer) number of words used to describe this application\n",
        "# (3) word vector (in list). A list representing the word vector (can be transformed to numpy ndarray afterwards), represent the claims of this application as a document vector.\n",
        "output_dataset_path = '/content/drive/MyDrive/EPFL/ML/ML-Patent_data/claim_with_vector/word_vector/word_vector.csv'\n",
        "\n",
        "# The path to our corpus file in the format of TaggedLineDocument as specified by gensim. Each line is all the claims of one application\n",
        "corpus_path = '/content/drive/MyDrive/EPFL/ML/ML-Patent_data/claim_with_vector/sampled/corpus_sampled.txt'\n",
        "# The path to the vocabulary file\n",
        "vocab_path = '/content/drive/MyDrive/EPFL/ML/ML-Patent_data/claim_with_vector/sampled/vocab_sampled.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPbYPDkXdqUD"
      },
      "source": [
        "## Prepare helper classes to generate handle large texts\n",
        "Since the number of texts are too large to be loaded into one single machine, we need to store our preprocessed texts in the corpus files and record its vocabulary set. So that the Doc2Vec model can train from these files without loading everything into memory, which not feasible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yt40p5zS2NIP"
      },
      "outputs": [],
      "source": [
        "class myVocab():\n",
        "  '''\n",
        "  Handle vocabulary for a language, contains all unique words seen in a corpus.\n",
        "  This class is used as a facilitation for training a Doc2Vec model\n",
        "  '''\n",
        "\n",
        "  def __init__(self, words:Iterable=None, path:str=None):\n",
        "    '''\n",
        "    Parameters\n",
        "    ------------\n",
        "    words: a list of words (in any iterable container) to be assigned as initial vocabulary\n",
        "    path: the file path indicates a corpus file, where each line is a word\n",
        "    '''\n",
        "    # Use frozenset instead of normal set can boost the performance\n",
        "    self.vocab = frozenset()\n",
        "\n",
        "    if words is not None:\n",
        "      self.add_vocab(words)\n",
        "\n",
        "    if path is not None:\n",
        "      self.load(path)\n",
        "    \n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.vocab)\n",
        "\n",
        "  def add_vocab(self, words:Iterable) -> None:\n",
        "    '''Given a new list of words (in any iterable container), merge them into our vocabulary set'''\n",
        "    self.vocab = self.vocab.union(frozenset(words))\n",
        "    # Exclude empty string\n",
        "    self.vocab -= {'', ' '}\n",
        "\n",
        "  def save(self, path:str, append:bool=False) -> None:\n",
        "    '''Save the current vocabulary set into an external file'''\n",
        "    # Decide whether to append at the end or overwrite as new file\n",
        "    open_mode = 'a' if append else 'w'\n",
        "\n",
        "    with open(path, open_mode) as vocab_file:\n",
        "      # Each line is an unique word\n",
        "      for word in self.vocab:\n",
        "        vocab_file.write(f'{word}\\n')\n",
        "\n",
        "  def load(self, path:str) -> None:\n",
        "    '''Load the previously saved vocabulary set file to contruct the object'''\n",
        "    with open(path, 'r') as vocab_file:\n",
        "      word = vocab_file.readline()\n",
        "      while word != '':\n",
        "        # Add the new word into our vocabulary\n",
        "        self.vocab.union(word)\n",
        "        # Each line is an unique word\n",
        "        word = vocab_file.readline()\n",
        "  \n",
        "  def build(self, path:str) -> None:\n",
        "    '''Build the vocabulary set from a document file'''\n",
        "    with open(path, 'r') as doc:\n",
        "      for line in doc:\n",
        "        # Remove the symbols that often attached with words: '.' ',' '?' '!' '(' ')' ':' ';' '/' '[' ']'\n",
        "        line = line.replace('.', '')\n",
        "        line = line.replace(',', '')\n",
        "        line = line.replace('?', '')\n",
        "        line = line.replace('!', '')\n",
        "        line = line.replace('(', '')\n",
        "        line = line.replace(')', '')\n",
        "        line = line.replace('[', '')\n",
        "        line = line.replace(']', '')\n",
        "        line = line.replace(':', '')\n",
        "        line = line.replace(';', '')\n",
        "        line = line.replace('/', ' ')\n",
        "        # Get each word\n",
        "        words = line.split()\n",
        "        # Add new vocabulary\n",
        "        self.add_vocab(words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w62O1Ws6djta"
      },
      "outputs": [],
      "source": [
        "class Claims():\n",
        "  '''A class to organize claims with their respective patent case.'''\n",
        "  def __init__(self):\n",
        "\n",
        "    # Dictionary using the application number to access the claims (in the respective order)\n",
        "    # Key: application number (in int64)\n",
        "    # Value: List of patent sentences (in the order of claim number)\n",
        "    self.claims = {}\n",
        "    \n",
        "    # Dictionary using the application number to access the word vector for claims\n",
        "    # Key: application number (in int64)\n",
        "    # Value: numpy array\n",
        "    self.word_vector = {}\n",
        "\n",
        "\n",
        "    # Dictionary using the application number to access the number of words used in claims\n",
        "    # Key: application number (in int64)\n",
        "    # Value: number of words used\n",
        "    self.word_count = {}\n",
        "\n",
        "  def reset(self):\n",
        "    '''Reset all the containers to empty'''\n",
        "    self.claims = {}\n",
        "    self.word_vector = {}\n",
        "    self.word_count = {}\n",
        "\n",
        "\n",
        "  def get_app_num(self):\n",
        "    '''Return the list of application numbers currently stored.'''\n",
        "    return list(self.claims.keys())\n",
        "\n",
        "  def get_claims(self, app_number:int) -> list:\n",
        "    '''Return the claims of the specified application number'''\n",
        "    return self.claims[app_number]\n",
        "\n",
        "  def get_word_vector(self, app_number:int) -> np.ndarray:\n",
        "    '''Return the word_vector of the specified application number'''\n",
        "    return self.word_vector[app_number]\n",
        "\n",
        "  def get_word_count(self, app_number:int) -> int:\n",
        "    '''Return the word_count of the specified application number'''\n",
        "    return self.word_count[app_number]\n",
        "\n",
        "\n",
        "  def set_new_claims(self, app_number:int, claims:list, model:gensim.models.doc2vec.Doc2Vec=None, count_word:bool=True) -> None:\n",
        "    '''Set a new entry'''\n",
        "    ## Assign the new claims\n",
        "    if app_number in self.claims:\n",
        "      self.claims[app_number] += claims\n",
        "    else:\n",
        "      self.claims[app_number] = claims\n",
        "\n",
        "    ## Assign the new word vector\n",
        "    if model is not None:\n",
        "      # Get the representation of claims in words \n",
        "      words = self.get_claim_words(app_number)\n",
        "      # Filter out the words which is not in our corpus\n",
        "      words = list(filter(lambda w: w in model.wv.key_to_index, words))\n",
        "      # Infer the vector by it's word representation\n",
        "      self.word_vector[app_number] = model.infer_vector(words)\n",
        "    \n",
        "    ## Assign the updated word_count\n",
        "    if count_word:\n",
        "      self.word_count[app_number] = len(words)\n",
        "\n",
        "\n",
        " \n",
        "  def add_claims(self, df:pd.DataFrame, column_names:Iterable=None, vocab:myVocab=None, sample_func:callable=None) -> None:\n",
        "    '''\n",
        "    Add the claims into our structure.\n",
        "\n",
        "    Parameters\n",
        "    ---------------\n",
        "    df: a dataframe which contains at least (1) application number (2) claim number (3) claim text\n",
        "    column_names: any ordered iterable containing the name of application number, claim number, and claim text in the dataframe df. \n",
        "                  If not specified, it's assumed that the first 3 columns are already the 3 columns mentioned above.\n",
        "    vocab: if specified, every time we examine a new sentence, it adds all new vocabulary into the vocab object.\n",
        "    sample_func: a callable function returns True or false. We process an application only if sample_func returns True\n",
        "    '''\n",
        "    # If not assigned, our target columns are assumed to be the first 3 columns\n",
        "    if column_names is None:\n",
        "      column_names = df.columns\n",
        "    # Obtain the correct column label for the dataframe\n",
        "    app_number_column_name   = column_names[0]\n",
        "    claim_number_column_name = column_names[1]\n",
        "    claim_text_column_name   = column_names[2]\n",
        "\n",
        "    # Get the unique list of application number\n",
        "    app_number_list = df[app_number_column_name].unique()\n",
        "\n",
        "    # Add claims to each application\n",
        "    for app in tqdm(app_number_list):\n",
        "\n",
        "      # If a sample function is defined, discard this application number if it returns false\n",
        "      if sample_func is not None:\n",
        "        if not sample_func():\n",
        "          continue\n",
        "\n",
        "      # Extract the sub-dataframe for current application number\n",
        "      df_current_app = df[ df[app_number_column_name] == app ]\n",
        "      # Sort the sub-dataframe by the claim number, so that we store the content in the correct order\n",
        "      df_current_app = df_current_app.sort_values(by=[claim_number_column_name])\n",
        "      # Extract the claims as list of claims\n",
        "      current_claims = df_current_app[claim_text_column_name].to_list()\n",
        "\n",
        "      # Store the extracted claims into the dictionary\n",
        "      if app in self.claims:\n",
        "        # If already exist such application, append new claims to the original one\n",
        "        self.claims[app] += current_claims\n",
        "      else:\n",
        "        # If there's no such application, assign the claims\n",
        "        self.claims[app] = current_claims\n",
        "      \n",
        "      # If vocabulary is specified, we add new words into the vocabulary set\n",
        "      if vocab is not None:\n",
        "        vocab.add_vocab(self.get_claim_words(app))\n",
        "\n",
        "\n",
        "\n",
        "  def get_claim_words(self, app_num:int) -> list:\n",
        "    '''\n",
        "    Return the claims in a list of words with respect to the assigned patent.\n",
        "    Return an empty list if such application number does not exist\n",
        "\n",
        "    Parameter\n",
        "    -------------\n",
        "    app_num: the application number\n",
        "    '''\n",
        "\n",
        "    # If such application number does not exist, return an empty list (not found)\n",
        "    if app_num not in self.claims:\n",
        "      return []\n",
        "\n",
        "    words = []\n",
        "    # Call the claims from the specified application\n",
        "    for sentence in self.claims[app_num]:\n",
        "      # Some times nan happens, but afterall this entry is meaningless if it's not a string\n",
        "      if type(sentence) != str:\n",
        "        continue\n",
        "\n",
        "      # Remove the symbols that often attached with words: '.' ',' '?' '!' '(' ')' ':' ';' '/' '[' ']'\n",
        "      sentence = self.__clear_special_symbols(sentence)\n",
        "\n",
        "      # Transform the sentence into a list of words\n",
        "      new_words = sentence.split()\n",
        "\n",
        "      # Append the new_words into the structure\n",
        "      words = words + new_words\n",
        "    \n",
        "    return words\n",
        "\n",
        "\n",
        "  def get_claim_as_sentence(self, app_num:int) -> str:\n",
        "    '''Collect all the claims and merge them into a single sentence.'''\n",
        "    # Initialize the sentence structure\n",
        "    sen = ''\n",
        "\n",
        "    # Call the claims from the specified application\n",
        "    for sentence in self.claims[app_num]:\n",
        "      # Some times nan happens, but afterall this entry is meaningless if it's not a string\n",
        "      if type(sentence) != str:\n",
        "        continue\n",
        "\n",
        "      # Remove the symbols that often attached with words: '.' ',' '?' '!' '(' ')' ':' ';' '/' '[' ']'\n",
        "      sentence = self.__clear_special_symbols(sentence)\n",
        "      \n",
        "      # Append the new sentence to the original sentence\n",
        "      sen += ' ' + sentence\n",
        "\n",
        "    return sen\n",
        "\n",
        "\n",
        "\n",
        "  def get_tagged_documents(self) -> list:\n",
        "    '''Return a list of documents as the training input for gensim.doc2vec'''\n",
        "    return list(self.tag_claims())\n",
        "\n",
        "\n",
        "  def tag_claims(self):\n",
        "    '''Tag all the claims to be trained'''\n",
        "    print('Tagging words for all the applications')\n",
        "    for ind, app_num in enumerate(tqdm(self.claims)):\n",
        "      # Get the representation of claims in list of words\n",
        "      words = self.get_claim_words(app_num)\n",
        "      yield gensim.models.doc2vec.TaggedDocument(words, [ind])\n",
        "\n",
        "\n",
        "  def assign_word_vector(self, model:gensim.models.doc2vec.Doc2Vec) -> None:\n",
        "    '''Assign word vectors to each application, and store it in self.word_vector as a dictionary'''\n",
        "    print('Assigning word vectors to claims of each application')\n",
        "    for app_num in tqdm(self.claims):\n",
        "      # Get the representation of claims in words \n",
        "      words = self.get_claim_words(app_num)\n",
        "      # Filter out the words which is not in our corpus\n",
        "      words = list(filter(lambda w: w in model.wv.key_to_index, words))\n",
        "      # Infer the vector by it's word representation\n",
        "      self.word_vector[app_num] = model.infer_vector(words)\n",
        "\n",
        "\n",
        "  def calculate_word_count(self) -> None:\n",
        "    '''Construct the word_counts for the existing claims'''\n",
        "    print(\"Calculating word counts...\")\n",
        "    for app in tqdm(self.claims):\n",
        "      # Get the representation of claim in individial words\n",
        "      words = self.get_claim_words(app)\n",
        "      # Store the respective word number\n",
        "      self.word_count[app] = len(words)\n",
        "      \n",
        "\n",
        "\n",
        "  \n",
        "  def save_all(self, path:str) -> None:\n",
        "    '''\n",
        "    Save the processed dataset as another csv file.\n",
        "\n",
        "    path: the path where the file is to be stored\n",
        "    '''\n",
        "\n",
        "    print(\"Saving the processed dataset...\")\n",
        "\n",
        "    with open(path, 'w') as ofile:\n",
        "      # Start a new csv file and initialize the columns\n",
        "      ofile.write('application_number,claims,word_vector,word_count\\n')\n",
        "\n",
        "      for app_num in tqdm(self.claims):\n",
        "        # Use 2 single quotes to represent the single quote in the sentence\n",
        "        claims_list = [ x.replace('\\'', '\\'\\'') for x in self.claims[app_num] if type(x) is str]\n",
        "        # Since double quotes have special meaning in .csv files, we change all the double quote representations into single quote\n",
        "        claim_list_in_str = str(claims_list).replace('\\\"', '\\'')\n",
        "        # Write the result and enclose list with double quotes\n",
        "        ofile.write(f\"{app_num},\\\"{claim_list_in_str}\\\",\\\"{self.word_vector[app_num].tolist()}\\\",{self.word_count[app_num]}\\n\")\n",
        "\n",
        "\n",
        "  def save_vector(self, path:str) -> None:\n",
        "    '''\n",
        "    Save the processed dataset as another csv file with (1) application number, (2) word_count (3) word_vector (in a list)\n",
        "\n",
        "    path: the path where the file should be stored\n",
        "    '''\n",
        "    print(\"Saving word count and word vector...\")\n",
        "\n",
        "\n",
        "    with open(path, 'w') as ofile:\n",
        "      # Start a new csv file and initialize the columns\n",
        "      ofile.write('application_number,word_count,word_vector\\n')\n",
        "\n",
        "      for app_num in tqdm(self.claims):\n",
        "        # Write the result and enclose list with double quotes\n",
        "        ofile.write(f\"{app_num},{self.word_count[app_num]},\\\"{self.word_vector[app_num].tolist()}\\\"\\n\")\n",
        "\n",
        "\n",
        "  def save_corpus(self, path:str, append:bool=False) -> None:\n",
        "    '''\n",
        "    Save the claims into a file with the TaggedLineDocument format specified by gensim Doc2Vec\n",
        "    \n",
        "    Parameters\n",
        "    -------------\n",
        "    path: the path including file name to save\n",
        "    append: True if want to append to the end to the existing file on 'path'. \n",
        "            False if want to overwrite the result\n",
        "    '''\n",
        "    print(\"Writing corpus to file...\")\n",
        "\n",
        "    # Decide whether to append at the end or overwrite as new file\n",
        "    open_mode = 'a' if append else 'w'\n",
        "\n",
        "    with open(path, open_mode) as corpus_file:\n",
        "      app_num_list = self.get_app_num()\n",
        "      # Each line represents a document (with unique tag)\n",
        "      # so in our case it's an application.\n",
        "      for app in tqdm(app_num_list):\n",
        "        corpus_file.write(self.get_claim_as_sentence(app)+'\\n')\n",
        "    \n",
        "  \n",
        "  def load(self, path:str, num_of_files:int) -> None:\n",
        "    '''\n",
        "    Load from the previously saved post-processed dataset\n",
        "    \n",
        "    Parameters\n",
        "    -------------\n",
        "    path: a path template, don't append the _[index] to the end. Just put [path_to_file]/[name].csv. The function will iterate through all the indices.\n",
        "    num_of_files: number of files to be loaded. The function automatically loads files with indices 0 to (num_of_files-1).\n",
        "    '''\n",
        "    print(\"Loading from the previously saved dataset\")\n",
        "\n",
        "    # Define the converters for data parsing\n",
        "    # all data is imported as strings, we do some extra handling to successfully convert the strings to lists.\n",
        "    #\n",
        "    # since we use 2 single quotes to represent the single quote in original text,\n",
        "    # we append a backslash '\\' before it so that we can parse it as a single quote in text, instead of a deliminator for strings with Python syntax\n",
        "    conv = {'claims': lambda x: ast.literal_eval(x.replace('\\'\\'', '\\\\\\'')),\n",
        "            'word_vector': lambda x: ast.literal_eval(x)}\n",
        "\n",
        "    # Just a trick to for the loop below\n",
        "    path = path.replace('.csv', '_-1.csv')\n",
        "\n",
        "    for file_index in range(num_of_files):\n",
        "      # Update the file index for each new file\n",
        "      path = path.replace(f'_{file_index-1}.csv', f'_{file_index}.csv')\n",
        "      with open(path) as infile:\n",
        "        print(f\"Loading processed data file {file_index}\")\n",
        "        df = pd.read_csv(path, converters=conv)\n",
        "\n",
        "        self.add_claims(df)\n",
        "\n",
        "  \n",
        "  def __clear_special_symbols(self, sentence:str) -> str:\n",
        "     '''\n",
        "     Remove the symbols that often attached with words: '.' ',' '?' '!' '(' ')' ':' ';' '/' '[' ']'\n",
        "     '''\n",
        "    sentence = sentence.replace('.', '')\n",
        "    sentence = sentence.replace(',', '')\n",
        "    sentence = sentence.replace('?', '')\n",
        "    sentence = sentence.replace('!', '')\n",
        "    sentence = sentence.replace('(', '')\n",
        "    sentence = sentence.replace(')', '')\n",
        "    sentence = sentence.replace('[', '')\n",
        "    sentence = sentence.replace(']', '')\n",
        "    sentence = sentence.replace(':', '')\n",
        "    sentence = sentence.replace(';', '')\n",
        "    sentence = sentence.replace('/', ' ')\n",
        "    return sentence\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0E2XrtD8jKA2"
      },
      "source": [
        "## Load from the data source for training\n",
        "We construct the corpus and vocabulary from the source data. So that we can train a Doc2Vec model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItPQV4UHJ51U"
      },
      "outputs": [],
      "source": [
        "# The root for the source file name. For example, the file can be like\n",
        "# \"{Path_to_datasets}/{NAME_ROOT}_000000000001.csv\"\n",
        "NAME_ROOT = 'features_claims_'\n",
        "\n",
        "# Maintain the vocabulary set with the claim data for training Doc2Vec\n",
        "vocab = myVocab()\n",
        "claims = Claims()\n",
        "\n",
        "## The below parts are only for training\n",
        "if train_new_model:\n",
        "\n",
        "  def app_sample_func():\n",
        "    '''Bernoulli r.v. with probability \"sample_prob\" for random sampling data '''\n",
        "    return np.random.rand() < sample_prob\n",
        "\n",
        "\n",
        "  for file_index in range(num_files):\n",
        "    # Format the file name\n",
        "    filename = f'{NAME_ROOT}{file_index:012}.csv'\n",
        "    path = os.path.join(source_path, filename)\n",
        "    with open(path) as patent_claim_file:\n",
        "      print(f\"Loading claim file \\'{filename}\\'. Current vocab size: {len(vocab)}\")\n",
        "      df = pd.read_csv(patent_claim_file)\n",
        "\n",
        "      claims.add_claims(df, vocab=vocab, sample_func=app_sample_func)\n",
        "\n",
        "    # Save corpus and vocabulary result to file (prevent session problem, allow us to have check points)\n",
        "    claims.save_corpus(corpus_path, append=True)\n",
        "    vocab.save(vocab_path)\n",
        "\n",
        "    # Reset the claim object to free up some RAM.\n",
        "    # Because claim texts are quite a large burden for memory :)\n",
        "    claims.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9qkIqSvjQvD"
      },
      "source": [
        "## Obtain the Model\n",
        "Either train a new model with the previously loaded data; or\n",
        "load an existing model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZY5ArFZCxGL",
        "outputId": "a92057b7-e6fe-4704-8f0b-2a6cc5f76df1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading from existing model from '/content/drive/MyDrive/EPFL/ML/ML-Patent_data/claim_with_vector/sampled/model_sampled.model'\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if train_new_model:\n",
        "  # Create the model object for training\n",
        "  model = gensim.models.doc2vec.Doc2Vec(min_count=model_min_count, vector_size=vector_size, epochs=model_epochs)\n",
        "\n",
        "  # Build vocabulary from the corpus file (in the format of TaggedLineDocument)\n",
        "  model.build_vocab(corpus_file=corpus_path)\n",
        "  # Train the model by the corpus file\n",
        "  model.train(corpus_file=corpus_path,\n",
        "              total_words=len(vocab),\n",
        "              sample=model_sample,\n",
        "              epochs=model.epochs)\n",
        "\n",
        "  # Save the model so that we can load it later without retrain it\n",
        "  model.save(model_path)\n",
        "\n",
        "else:\n",
        "  print(f'Loading from existing model from \\'{model_path}\\'')\n",
        "  model = gensim.models.doc2vec.Doc2Vec.load(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOryN5Lai5Wy"
      },
      "source": [
        "## Apply word vector\n",
        "Once having a good Doc2Vec model, we now apply the model to infer word vector for each application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JY1U41avJrZk",
        "outputId": "f05395da-3cbb-4cb5-c424-877d960e2a42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading claim file 'features_claims_000000000040.csv'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 46523/46523 [01:48<00:00, 427.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assigning word vectors to claims of each application\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 46523/46523 [19:14<00:00, 40.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating word counts...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 46523/46523 [00:07<00:00, 6277.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving word count and word vector...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 46523/46523 [00:21<00:00, 2206.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading claim file 'features_claims_000000000041.csv'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 48667/48667 [01:57<00:00, 415.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assigning word vectors to claims of each application\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 48667/48667 [19:30<00:00, 41.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating word counts...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 48667/48667 [00:07<00:00, 6656.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving word count and word vector...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 48667/48667 [00:22<00:00, 2177.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading claim file 'features_claims_000000000042.csv'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 45525/45525 [01:43<00:00, 438.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assigning word vectors to claims of each application\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 45525/45525 [17:44<00:00, 42.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating word counts...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 45525/45525 [00:06<00:00, 6564.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving word count and word vector...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 45525/45525 [00:20<00:00, 2185.52it/s]\n"
          ]
        }
      ],
      "source": [
        "# Determine the file index to start loading\n",
        "file_from = 0\n",
        "\n",
        "# Generate the file path with indices\n",
        "output_path = output_dataset_path.replace('.csv', f'_{file_from-1}.csv')\n",
        "\n",
        "claims = Claims()\n",
        "\n",
        "for file_index in range(file_from, num_files):\n",
        "  # Format the file name\n",
        "  filename = f'{NAME_ROOT}{file_index:012}.csv'\n",
        "  path = os.path.join(source_path, filename)\n",
        "  with open(path) as patent_claim_file:\n",
        "    print(f\"Loading claim file \\'{filename}\\'\")\n",
        "    df = pd.read_csv(patent_claim_file)\n",
        "\n",
        "    claims.add_claims(df)\n",
        "\n",
        "    # Infer the word vector for the claims\n",
        "    claims.assign_word_vector(model)\n",
        "\n",
        "    # Calculate the word count\n",
        "    claims.calculate_word_count()\n",
        "\n",
        "    # Update the path index and save the processed file\n",
        "    output_path = output_path.replace(f'_{file_index-1}.csv', f'_{file_index}.csv')\n",
        "    claims.save_vector(output_path)\n",
        "\n",
        "    claims.reset()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Claim2Vec_sample.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
